"""
LLM Agent V2 - ä½¿ç”¨æœ¬åœ°LLM + LoRAåœ¨çº¿å­¦ä¹ 
"""
from typing import Dict, Any, Optional, List, Tuple
import time
import numpy as np

try:
    from core.model_loader import SharedModelPool
except ImportError:
    from core.mock_model import MockSharedModelPool as SharedModelPool
    print("Using Mock Model (no real LLM needed)")

from core.lora_trainer import LoRAAdapter
from core.world import SimpleWorld
from config import Config


class LLMAgentV2:
    """åŸºäºæœ¬åœ°å¾®å‹LLMçš„æ™ºèƒ½ä½“ï¼ˆå¸¦LoRAåœ¨çº¿å­¦ä¹ ï¼‰"""
    
    # ç±»å˜é‡ï¼šå…±äº«çš„æ¨¡å‹æ± ï¼ˆ20ä¸ªagentå…±ç”¨ä¸€ä¸ªbase modelï¼‰
    _model_pool = SharedModelPool()
    
    def __init__(self, agent_id: int, position: Tuple[int, int], energy: int = 200, lora: Optional[LoRAAdapter] = None):
        self.agent_id = agent_id
        self.position = position  # (x, y)
        self.energy = energy
        self.max_energy = 200
        self.carrying_food = False
        self.alive = True
        
        # LoRAé€‚é…å™¨ï¼ˆæ¯ä¸ªagentçš„ä¸ªæ€§åŒ–"çŸ¥è¯†"ï¼‰
        self.lora = lora or LoRAAdapter(agent_id)
        
        # å†å²è®°å½•
        self.action_history: List[str] = []
        self.observation_history: List[str] = []
        self.reward_history: List[float] = []
        
        # ğŸ§  æ¢ç´¢å’Œè®°å¿†ç³»ç»Ÿ
        self.explored_cells: set = set()  # å·²æ¢ç´¢çš„ä½ç½®
        self.food_memory: List[Tuple[int, int]] = []  # è®°ä½çš„é£Ÿç‰©ä½ç½®ï¼ˆæœ€å¤š10ä¸ªï¼‰
        self.path_history: List[Tuple[int, int]] = []  # æœ€è¿‘çš„è·¯å¾„ï¼ˆæœ€å¤š50æ­¥ï¼‰
        self.successful_paths: List[List[Tuple[int, int]]] = []  # æˆåŠŸçš„è·¯å¾„è®°å½•
        self.danger_cells: set = set()  # å±é™©åŒºåŸŸï¼ˆæ’å¢™/å›°ä½çš„åœ°æ–¹ï¼‰
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_reward = 0.0
        self.food_collected = 0
        self.food_delivered = 0
        self.steps_taken = 0
        self.decision_times = []
        self.exploration_rate = 0.0  # æ¢ç´¢ç‡
        
    def observe(self, world: SimpleWorld) -> Dict[str, Any]:
        """è§‚å¯Ÿå‘¨å›´ç¯å¢ƒ"""
        x, y = self.position
        nearby = world.get_nearby_info(x, y, radius=1)
        
        # ğŸ§  æ›´æ–°æ¢ç´¢è®°å½•
        self.explored_cells.add((x, y))
        
        # ğŸ§  æ›´æ–°é£Ÿç‰©è®°å¿†
        for dir_name, (nx, ny) in [("north", (x, y+1)), ("south", (x, y-1)), 
                                     ("east", (x+1, y)), ("west", (x-1, y))]:
            if nearby.get(f"food_{dir_name}"):
                if (nx, ny) not in self.food_memory:
                    self.food_memory.append((nx, ny))
                    if len(self.food_memory) > 10:  # æœ€å¤šè®°ä½10ä¸ª
                        self.food_memory.pop(0)
        
        observation = {
            "position": self.position,
            "energy": self.energy,
            "carrying_food": self.carrying_food,
            "nearby": nearby,
            "alive": self.alive,
            "explored_nearby": self._count_explored_nearby(x, y),
            "remembered_food": len(self.food_memory),
        }
        
        return observation
    
    def _count_explored_nearby(self, x: int, y: int, radius: int = 3) -> int:
        """ç»Ÿè®¡å‘¨å›´å·²æ¢ç´¢çš„æ ¼å­æ•°"""
        count = 0
        total = 0
        for dy in range(-radius, radius+1):
            for dx in range(-radius, radius+1):
                total += 1
                if (x+dx, y+dy) in self.explored_cells:
                    count += 1
        return count / total if total > 0 else 0.0
    
    def _format_observation(self, obs: Dict[str, Any]) -> str:
        """å°†è§‚å¯Ÿæ ¼å¼åŒ–ä¸ºç®€æ´çš„æ–‡æœ¬promptï¼ˆåŒ…å«è®°å¿†å’Œæ¢ç´¢ä¿¡æ¯ï¼‰"""
        x, y = obs["position"]
        energy = obs["energy"]
        carrying = obs["carrying_food"]
        nearby = obs["nearby"]
        
        # æ„å»ºç®€æ´çš„çŠ¶æ€æè¿°ï¼ˆæ§åˆ¶tokenæ•°ï¼‰
        prompt = f"At ({x},{y}), E{energy}"
        
        if carrying:
            prompt += ", has food"
        
        # ğŸ§  æ·»åŠ æ¢ç´¢ä¿¡æ¯
        explored = obs.get("explored_nearby", 0)
        if explored < 0.3:
            prompt += ", unexplored area"
        elif explored > 0.8:
            prompt += ", familiar area"
        
        # ğŸ§  æ·»åŠ è®°å¿†ä¿¡æ¯
        remembered = obs.get("remembered_food", 0)
        if remembered > 0 and not carrying:
            prompt += f", remember {remembered} food"
        
        # æè¿°å‘¨å›´ç¯å¢ƒï¼ˆåªè¯´é‡è¦çš„ï¼‰
        directions = []
        if nearby.get("food_north"):
            directions.append("foodâ†‘")
        if nearby.get("food_south"):
            directions.append("foodâ†“")
        if nearby.get("food_east"):
            directions.append("foodâ†’")
        if nearby.get("food_west"):
            directions.append("foodâ†")
        
        if nearby.get("home_north"):
            directions.append("homeâ†‘")
        if nearby.get("home_south"):
            directions.append("homeâ†“")
        if nearby.get("home_east"):
            directions.append("homeâ†’")
        if nearby.get("home_west"):
            directions.append("homeâ†")
        
        if nearby.get("wall_north"):
            directions.append("wallâ†‘")
        if nearby.get("wall_south"):
            directions.append("wallâ†“")
        if nearby.get("wall_east"):
            directions.append("wallâ†’")
        if nearby.get("wall_west"):
            directions.append("wallâ†")
        
        if directions:
            prompt += f". See: {' '.join(directions)}"
        
        return prompt
    
    def decide_action(self, observation: Dict[str, Any]) -> str:
        """ä½¿ç”¨LLMå†³å®šè¡ŒåŠ¨"""
        # æ ¼å¼åŒ–prompt
        obs_text = self._format_observation(observation)
        prompt = f"{obs_text}. Action:"
        
        # æ·»åŠ ç®€çŸ­çš„å†å²ä¸Šä¸‹æ–‡
        if len(self.action_history) > 0:
            recent = self.action_history[-2:]
            prompt = f"[{','.join(recent)}] {prompt}"
        
        # ä½¿ç”¨å…±äº«æ¨¡å‹ç”Ÿæˆ
        start_time = time.time()
        base_model = self._model_pool.get_base_model()
        
        # TODO: åº”è¯¥åº”ç”¨ä¸ªæ€§åŒ–LoRAæƒé‡ï¼Œæš‚æ—¶ç›´æ¥ç”¨base model
        raw_action = base_model.generate(prompt, max_tokens=10, temperature=0.7)
        
        decision_time = time.time() - start_time
        self.decision_times.append(decision_time)
        
        # æ ‡å‡†åŒ–action
        action = self._normalize_action(raw_action)
        
        return action
    
    def _normalize_action(self, raw_action: str) -> str:
        """ä»LLMè¾“å‡ºæå–æ ‡å‡†action"""
        action = raw_action.lower().strip()
        
        # æå–å…³é”®è¯
        if "north" in action or "up" in action or "â†‘" in action:
            return "move_north"
        elif "south" in action or "down" in action or "â†“" in action:
            return "move_south"
        elif "east" in action or "right" in action or "â†’" in action:
            return "move_east"
        elif "west" in action or "left" in action or "â†" in action:
            return "move_west"
        elif "pick" in action or "collect" in action or "take" in action:
            return "pick_food"
        elif "drop" in action or "store" in action or "deliver" in action:
            return "drop_food"
        else:
            # é»˜è®¤éšæœºç§»åŠ¨
            return np.random.choice(["move_north", "move_south", "move_east", "move_west"])
    
    def execute_action(self, action: str, world: SimpleWorld) -> float:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›å¥–åŠ±"""
        reward = 0.0
        x, y = self.position
        old_pos = (x, y)
        
        if action == "move_north":
            new_pos = (x, y + 1)
        elif action == "move_south":
            new_pos = (x, y - 1)
        elif action == "move_east":
            new_pos = (x + 1, y)
        elif action == "move_west":
            new_pos = (x - 1, y)
        elif action == "pick_food":
            if world.has_food(x, y) and not self.carrying_food:
                world.remove_food(x, y)
                self.carrying_food = True
                self.food_collected += 1
                reward = 10.0
                # ğŸ§  è®°å½•æˆåŠŸè·¯å¾„
                if len(self.path_history) > 0:
                    self.successful_paths.append(self.path_history.copy())
                    if len(self.successful_paths) > 5:
                        self.successful_paths.pop(0)
                # ä»è®°å¿†ä¸­ç§»é™¤è¿™ä¸ªé£Ÿç‰©
                if (x, y) in self.food_memory:
                    self.food_memory.remove((x, y))
            else:
                reward = -1.0
            return reward
        elif action == "drop_food":
            if self.carrying_food and world.is_home(x, y):
                world.add_stored_food()
                self.carrying_food = False
                self.food_delivered += 1
                reward = 20.0
                # ğŸ§  è®°å½•è¶…çº§æˆåŠŸè·¯å¾„
                if len(self.path_history) > 0:
                    self.successful_paths.append(self.path_history.copy())
            else:
                reward = -1.0
            return reward
        else:
            new_pos = (x + np.random.randint(-1, 2), y + np.random.randint(-1, 2))
            reward = -2.0
        
        # ç§»åŠ¨
        if world.can_move(*new_pos):
            self.position = new_pos
            
            # ğŸ§  æ›´æ–°è·¯å¾„è®°å¿†
            self.path_history.append(new_pos)
            if len(self.path_history) > 50:
                self.path_history.pop(0)
            
            # ğŸ§  æ¢ç´¢å¥–åŠ±
            if new_pos not in self.explored_cells:
                reward += 0.5  # æ¢ç´¢æ–°åŒºåŸŸå¥–åŠ±
            
            reward += -0.2  # ç§»åŠ¨æˆæœ¬
            
            if world.has_food(*new_pos):
                reward += 3.0
            elif world.is_home(*new_pos) and self.carrying_food:
                reward += 5.0
        else:
            # ğŸ§  è®°å½•å±é™©åŒºåŸŸ
            self.danger_cells.add(new_pos)
            reward = -3.0
        
        return reward
    
    def step(self, world: SimpleWorld) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€æ­¥å®Œæ•´å¾ªç¯"""
        if not self.alive:
            return {"action": "dead", "reward": 0.0, "energy": 0, "alive": False}
        
        # 1. è§‚å¯Ÿ
        observation = self.observe(world)
        obs_text = self._format_observation(observation)
        
        # 2. å†³ç­–
        action = self.decide_action(observation)
        
        # 3. æ‰§è¡Œ
        reward = self.execute_action(action, world)
        
        # 4. ä¸‹ä¸€ä¸ªè§‚å¯Ÿ
        next_observation = self.observe(world)
        next_obs_text = self._format_observation(next_observation)
        
        # 5. è®°å½•ç»éªŒï¼ˆç”¨äºåœ¨çº¿å­¦ä¹ ï¼‰
        self.lora.add_experience(obs_text, action, reward, next_obs_text)
        
        # 6. åœ¨çº¿å­¦ä¹ ï¼ˆæ¯10æ­¥è®­ç»ƒä¸€æ¬¡ï¼‰
        if Config.ENABLE_ONLINE_LEARNING and self.steps_taken % 10 == 0:
            if self.lora.should_train():
                self.lora.train_step()
        
        # 7. æ›´æ–°èƒ½é‡
        self.energy -= 1
        if self.energy <= 0:
            self.alive = False
            self.energy = 0
        
        # 8. æ›´æ–°ç»Ÿè®¡
        self.observation_history.append(obs_text)
        self.action_history.append(action)
        self.reward_history.append(reward)
        self.total_reward += reward
        self.steps_taken += 1
        
        # ğŸ§  æ›´æ–°æ¢ç´¢ç‡
        self.exploration_rate = len(self.explored_cells) / (self.steps_taken + 1)
        
        return {
            "action": action,
            "reward": reward,
            "energy": self.energy,
            "position": self.position,
            "alive": self.alive,
            "total_reward": self.total_reward,
            "explored": len(self.explored_cells),
            "memories": len(self.food_memory),
        }
    
    def get_fitness(self) -> float:
        """è®¡ç®—é€‚åº”åº¦ï¼ˆç”¨äºè¿›åŒ–é€‰æ‹©ï¼‰"""
        fitness = 0.0
        fitness += self.total_reward * 1.0
        fitness += self.food_collected * 5.0
        fitness += self.food_delivered * 10.0
        fitness += self.steps_taken * 0.1
        
        if self.steps_taken > 0:
            efficiency = self.food_delivered / (self.steps_taken + 1)
            fitness += efficiency * 50.0
        
        return max(0.0, fitness)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        avg_decision_time = np.mean(self.decision_times) if self.decision_times else 0.0
        avg_reward = np.mean(self.reward_history) if self.reward_history else 0.0
        
        return {
            "agent_id": self.agent_id,
            "alive": self.alive,
            "energy": self.energy,
            "position": self.position,
            "fitness": self.get_fitness(),
            "total_reward": self.total_reward,
            "avg_reward": avg_reward,
            "food_collected": self.food_collected,
            "food_delivered": self.food_delivered,
            "steps": self.steps_taken,
            "avg_decision_ms": avg_decision_time * 1000,
            "lora_stats": self.lora.get_stats()
        }
    
    @classmethod
    def breed(cls, parent1: 'LLMAgentV2', parent2: 'LLMAgentV2', child_id: int, position: Tuple[int, int]) -> 'LLMAgentV2':
        """ç¹æ®–ï¼šåˆå¹¶ä¸¤ä¸ªparentçš„LoRAæƒé‡"""
        # è®¡ç®—æƒé‡æ¯”ä¾‹
        fitness1 = parent1.get_fitness()
        fitness2 = parent2.get_fitness()
        total_fitness = fitness1 + fitness2 + 1e-6
        ratio1 = fitness1 / total_fitness
        
        # åˆå¹¶LoRA
        child_lora = parent1.lora.merge_with(parent2.lora, ratio=ratio1)
        child_lora.agent_id = child_id
        
        # åˆ›å»ºå­ä»£
        child = cls(child_id, position, energy=200, lora=child_lora)
        
        return child
